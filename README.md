# transformer_language_models
Built various transformer architectures from scratch, including the classic encoder- decoder transformer for machine translation and the GPT-like decoder-only transformer for language modeling.


1) Sample text generated from a 9.5M GPT-like decoder-only autoregressive language model, trained on the Tiny Shakespeare dataset:

![posterior corner plot](https://raw.githubusercontent.com/hschia/transformer_language_models/main/GPT-like_decoder_only_transformer/sample_generated_text.png)


2) Sample translation from English text to French text, generated by a small 1.5M encoder-decoder transformer:

![posterior corner plot](https://raw.githubusercontent.com/hschia/transformer_language_models/main/encoder_decoder_machine_translation/sample_text.png) 
   
