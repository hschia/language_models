# transformer_language_models
Building various transformer architectures from scratch, including the classic encoder- decoder transformer for machine translation and the GPT-like decoder-only transformer for language modeling.


1) Sample text from a 9.5M GPT-like decoder-only autoregressive language model, trained on the Tiny Shakespeare dataset:

![posterior corner plot](https://raw.githubusercontent.com/hschia/transformer_language_models/main/GPT-like_decoder_only_transformer/sample_generated_text.png)
